{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: In this example, we will scrape the historic Deliveroo pages archived\n",
    "  by the Wayback Machine at the Internet Archive for pages that filter Deliveroo Editions\n",
    "  facilities.\n",
    "output-file: wayback.html\n",
    "title: Wayback\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following URL searches the Internet Archive for all archived Deliveroo webpages, including search results. With selenium, we can visit this page and filter the results to get all the URLs containing the \"deliveroo+editions\" filter by adding this text to an input HTML element on the Internet Archive site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://web.archive.org/web/*/https://deliveroo.co.uk/restaurants/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go ahead an start driving a Chrome Browser. We will set headless to `True` so we won't be able to see the browser. You can set `headless=False` if you'd like to view the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "driver = initialise_driver(service, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "driver.get(base_url)\n",
    "wait = WebDriverWait(driver, 20)    \n",
    "filter_input = wait.until(EC.presence_of_element_located((By.ID, 'resultsUrl_filter')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function didn't return an error then we know the element successfully loaded. Now we'd like to interact with a child of the `resultsUrl_filter` element and submit the `deliveroo+editions` filter to get the relevant results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "input_element = filter_input.find_element(By.TAG_NAME, 'input')\n",
    "input_element.clear()\n",
    "input_element.send_keys('deliveroo+editions')\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now grab all the results from the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_urls = []\n",
    "while not editions_urls: \n",
    "    table = driver.find_element(By.ID, 'resultsUrl')\n",
    "    cells = table.find_elements(By.CLASS_NAME, 'url')\n",
    "    editions_urls = []\n",
    "    for td in table.find_elements(By.CSS_SELECTOR, 'td.url'):\n",
    "        url = td.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "        editions_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# check that table results were scraped:\n",
    "assert editions_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print all the archived Deliveroo search results filtering for editions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_urls[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Captures\n",
    "\n",
    "For each of these historic URLs, we will now get all the captures for each. With these captures we can view versions of these webpages that have been captured over time and extract data from them. Let's try this with an example url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_urls[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the number of captures including the first and last capture are included conveniently within an element with `class_name=captures-range-info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "driver.get(editions_urls[5])\n",
    "wait = WebDriverWait(driver, 20)    \n",
    "range_info = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"captures-range-info\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert range_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "range_info.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are captures between 2020 and 2021. Lets extract these years programmatically: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_urls[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def get_range_info(url:str,  # Wayback calendar view URL\n",
    "                  ):\n",
    "    \"Returns the range of years for which url captures exist.\"\n",
    "    driver = initialise_driver(service,True)\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)    \n",
    "    range_info = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"captures-range-info\")))\n",
    "    capture_links = range_info.find_elements(By.CSS_SELECTOR, 'a')\n",
    "    oldest_capture = capture_links[0].get_attribute('href')\n",
    "    latest_capture = capture_links[1].get_attribute('href') \n",
    "    start_timestamp = oldest_capture.split('/')[4]\n",
    "    end_timestamp = latest_capture.split('/')[4]   \n",
    "    start_year = start_timestamp[:4]\n",
    "    end_year = end_timestamp[:4]\n",
    "    return start_year, end_year, driver\n",
    "\n",
    "start_year, end_year, driver = get_range_info(editions_urls[5])\n",
    "print(f\"Captures between {start_year} and {end_year}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "assert start_year == \"2020\"\n",
    "assert end_year == \"2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get every capture between these years from the calendar UI. To access the html with captures for a given year, we need to find the clickable element representing that year with `class=sparkline-year-label` and click on this element to load the html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "year = start_year\n",
    "year_selector = driver.find_element(By.XPATH, f'//*[contains(@class, \"sparkline-year-label\") and text()=\"{year}\"]')\n",
    "assert year_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "year_selector.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10) \n",
    "calendar = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"calendar-grid\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "captures = calendar.find_elements(By.CSS_SELECTOR, 'a')\n",
    "capture_urls = []\n",
    "for capture in captures:\n",
    "    capture_urls.append(capture.get_attribute('href'))\n",
    "assert capture_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got all the URLs for a single year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "capture_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now get every capture for every year, for every url: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may encounter a slight hiccup for all editions_urls with only 1 capture as the url will take us to the capture page rather than the calendar view, therefore we should try to find \"capture-range-info\" and if not just add the url to our capture_urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "capture_urls = []\n",
    "for i, url in enumerate(tqdm(editions_urls)):\n",
    "    if \"*\" in url: \n",
    "        attempts = 0\n",
    "        while attempts < 2: \n",
    "            try:\n",
    "                start_year, end_year, driver = get_range_info(url)\n",
    "                for year in range(int(start_year), int(end_year)+1):\n",
    "                    year_selector = driver.find_element(By.XPATH, f'//*[contains(@class, \"sparkline-year-label\") and text()=\"{year}\"]')\n",
    "                    year_selector.click()\n",
    "                    wait = WebDriverWait(driver, 10) \n",
    "                    calendar = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"calendar-grid\")))\n",
    "                    captures = calendar.find_elements(By.CSS_SELECTOR, 'a')\n",
    "                    for capture in captures:\n",
    "                        capture_urls.append(capture.get_attribute('href'))\n",
    "                        attempts = 2\n",
    "                        break\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                attempts += 1\n",
    "                time.sleep(1)\n",
    "                print(url)\n",
    "    else:\n",
    "        capture_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "capture_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save these urls to a file for access later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"data/capture_urls.json\", 'w') as f:\n",
    "#     json.dump(capture_urls, f) \n",
    "\n",
    "with open(\"data/capture_urls.json\", 'r') as f:\n",
    "    capture_urls = json.load(f)\n",
    "\n",
    "# print(capture_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the captures, we would now like to visit the url and extract all restaurant information. On each URL, there is a grid of restaurants within a `<ul>` element containing the `HomeFeedGrid` substring within the class name. We want to first locate all the `<li>` elements within `<ul>` without any parent `<li>` elements as this will give us list items corresponding to each restuarant. For this we can use [`get_restaurants`](https://ribenamaplesyrup.github.io/deliveroo-editions/deliveroo_utils.html#get_restaurants):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# driver= initialise_driver(service,True)\n",
    "url = capture_urls[0]\n",
    "filtered_tags = get_restaurant_tags(url)\n",
    "len(filtered_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the number of `<li>` elements within `filtered_li_tags`, we should find it equates to the number of restuarants listed on the webpage. Next we want to iterate through these items, locating another unordered list element containing 3 list items (restuarant name, description and delivery details). The element may also contain a link to the restaurant's Deliveroo url. We can use [`get_restaurants`](https://ribenamaplesyrup.github.io/deliveroo-editions/deliveroo_utils.html#get_restaurants) to get both the tags from before and the metadata all in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "get_restaurants(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it altogether, lets iterate through the capture urls getting the metadata for each restaurant in each capture. We also need to be careful of rate limits as Wayback limits us to [15 retrieval's per minute](https://en.wikipedia.org/wiki/Wayback_Machine#:~:text=Starting%20in%20October%202019%2C%20users,requests%20and%20retrievals%20per%20minute.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions = []\n",
    "driver = initialise_driver(service,True)\n",
    "retrievals = 0\n",
    "for i, capture in enumerate(tqdm(capture_urls)):\n",
    "    # API limit of 15 retrieval's per minute\n",
    "    attempts = 0\n",
    "    while attempts < 2:\n",
    "        try:\n",
    "            metadata = get_restaurants(capture, driver)\n",
    "            timestamped_metadata = add_timestamps_to_restaurants(metadata, capture)\n",
    "            editions += metadata\n",
    "            retrievals += 1\n",
    "            attempts = 2\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # reinitialise the driver if error:\n",
    "            driver.close()\n",
    "            driver = initialise_driver(service,True)\n",
    "            attempts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now process the data such that `timestamp` and `timestamp_url` are grouped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_list = []\n",
    "for edition in editions:\n",
    "    if editions_list:\n",
    "        found=False\n",
    "        index = \"\"\n",
    "        for i,d in enumerate(editions_list):\n",
    "            if d.get('name') == edition['name'] and d.get('location') == edition['location'] and edition['timestamp'] not in d.get('timestamps'):\n",
    "                index = i\n",
    "                found=True\n",
    "                break\n",
    "        if found:\n",
    "            editions_list[index]['timestamps'].append(edition['timestamp'])\n",
    "            editions_list[index]['timestamp_urls'].append(edition['timestamp_url'])\n",
    "        else:\n",
    "            editions_list.append({'name': edition['name'], 'location': edition['location'], 'timestamps': [edition['timestamp']], 'timestamp_urls': [edition['timestamp_url']], 'restaurant_url': edition['restaurant_url']})\n",
    "    else: \n",
    "        editions_list.append({'name': editions[0]['name'], 'location': editions[0]['location'], 'timestamps': [editions[0]['timestamp']], 'timestamp_urls': [editions[0]['timestamp_url']], 'restaurant_url': editions[0]['restaurant_url']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_wayback_df = pd.DataFrame.from_dict(editions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "editions_wayback_df.to_csv('data/editions_wayback.csv', encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
