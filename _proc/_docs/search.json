[
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "deliveroo-editions",
    "section": "Install",
    "text": "Install\npip install deliveroo_editions"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "deliveroo-editions",
    "section": "Usage",
    "text": "Usage\nThis repo includes notebook examples in nbs demonstrating how to scrape current and historic on Deliveroo’s dark kitchens from Deliveroo’s current webpages and captures of historic webpages.\n\n\n\n\n\n\n\n\n\nTutorial\nTitle\nDescription\nFormat\n\n\n\n\n00\nWayback\nGet all ‘editions’ kitchens from historic Deliveroo webpages archived via the Internet Archive’s Wayback Machine.\nJupyter Notebook\n\n\n01\nFSA\nGet all ‘editions’ kitchens based at or near FSA hygiene rated facilities with ‘Deliveroo’ in their name.\nJupyter Notebook"
  },
  {
    "objectID": "fsa.html",
    "href": "fsa.html",
    "title": "FSA",
    "section": "",
    "text": "import requests \nfrom bs4 import BeautifulSoup\nfrom nbdev.showdoc import *\nfrom deliveroo_editions.selenium_utils import *\nfrom deliveroo_editions.deliveroo_utils import *\nimport pandas as pd\nimport datetime\n\nWe can find all of Deliveroo’s rated kitchens at the following url:\n\nurl = \"https://ratings.food.gov.uk/enhanced-search/en-GB/%22deliveroo%22/%5E/Relevance/0/%5E/%5E/0/1/50\"\n\nLets parse the webpage and get the addresses for each of the restaurants listed:\n\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    html_content = response.content\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n\nbusinesses = soup.find_all('div', class_='ResultsBusinessDetails')\n\n\ntype(businesses[0])\n\nbs4.element.Tag\n\n\n\ndef extract_address(business:\"bs4.element.Tag\"  # div of class='ResultsBusinessDetails'\n                   ):\n                       \"Extracts address string from div\"\n                       return business.find('div', class_='ResultsBusinessAddress').text.strip().replace('\\r\\n', ' ')\n\ndef extract_postcode(business:\"bs4.element.Tag\"  # div of class='ResultsBusinessDetails'\n                    ):\n                        \"Extracts address string from div\"\n                        return business.find('div', class_='ResultsBusinessPostcode').text.strip().replace('\\r\\n', ' ')\n\n\naddresses = [{\"address\": extract_address(business), \"postcode\": extract_postcode(business)} for business in businesses]\nassert len(addresses) == 31\n\n\naddresses\n\n[{'address': 'Unit 2 Editions Battersea 15a Parkfield Industrial Estate Culvert Place London',\n  'postcode': 'SW115BA'},\n {'address': 'Deliveroo Unit 4 Roman Way Industrial Estate 149 Roman Way Islington',\n  'postcode': 'N7 8XH'},\n {'address': 'Unit 3-4 Restwell House Coldhams Road  Cambridge Cambridgeshire',\n  'postcode': 'CB1 3EW'},\n {'address': 'Unit 1 Saxon Works 22 Olive Road Hove', 'postcode': 'BN3 7GY'},\n {'address': 'Unit 1 Scott Hall Mills Scott Hall Street Meanwood Leeds',\n  'postcode': 'LS7 2HT'},\n {'address': '20 Acton Park Estate The Vale Acton Ealing',\n  'postcode': 'W3 7QE'},\n {'address': '33 Acton Park Estate The Vale Acton London',\n  'postcode': 'W3 7QE'},\n {'address': 'Unit 1 Scott Hall Mills Scott Hall Street Meanwood',\n  'postcode': 'LS7 2HT'},\n {'address': '86 North Western Street Ardwick Manchester',\n  'postcode': 'M12 6DY'},\n {'address': 'Railway Arches 89 To 90  Enid Street London',\n  'postcode': 'SE16 3RA'},\n {'address': 'Ground Floor and Basement and Rear Yard 70 New Oxford Street  London',\n  'postcode': 'WC1A 1EU'},\n {'address': 'Units 5-7 Yorkton Street Hoxton  Hackney London',\n  'postcode': 'E2 8NH'},\n {'address': 'Unit 12 St Josephs Business Park St Josephs Close Hove',\n  'postcode': 'BN3 7HG'},\n {'address': 'Units 29 To 30 Clifton Road  Cambridge Cambridgeshire',\n  'postcode': 'CB1 7EB'},\n {'address': 'Arch 114 Randall Road London LONDON', 'postcode': 'SE11 5JR'},\n {'address': 'Unit 2 271 Merton Road London', 'postcode': 'SW18 5JS'},\n {'address': '145 Ormside Street London', 'postcode': 'SE15 1TF'},\n {'address': '115 Finchley Road  London', 'postcode': 'NW3 6HY'},\n {'address': 'Unit 1 Redwood Court Salisbury Street  Nottingham',\n  'postcode': 'NG7 2BQ'},\n {'address': 'Unit 3 Crescent Court Business Centre 4C North Crescent Canning Town',\n  'postcode': 'E16 4TG'},\n {'address': '2 Phoenix Industrial Estate Apsley Way London',\n  'postcode': 'NW2 7LN'},\n {'address': 'Unit 21 Cranford Way Hornsey London', 'postcode': 'N8 9DG'},\n {'address': 'Unit 6 Great Cambridge Industrial Estate Lincoln Road Enfield',\n  'postcode': 'EN1 1SH'},\n {'address': 'Unit 1 Glenfrome House Eastgate Road Eastville Bristol',\n  'postcode': 'BS5 6XX'},\n {'address': 'Units 3-4 Dulwich Business Centre Malham Road Forest Hill',\n  'postcode': 'SE23 1AG'},\n {'address': 'Cannon Bridge House 1 Cousin Lane London',\n  'postcode': 'EC4R 3TE'},\n {'address': '21-23 Oxford Street St Philips Bristol', 'postcode': 'BS2 0QT'},\n {'address': 'Unit 24 Mitre Bridge Industrial Park Mitre Way London',\n  'postcode': 'W10 6AU'},\n {'address': '1 Scotland Street Glasgow', 'postcode': 'G5 8LS'},\n {'address': 'Unit 4 271 Merton Road London', 'postcode': 'SW18 5JS'},\n {'address': 'Unit 1 Saxon Works 22 Olive Road Hove', 'postcode': 'BN3 5LE'}]\n\n\nNow we have the ~30 addresses, we can search Deliveroo’s site for all the editions kitchens based at these addresses:\n\naddress_strings = [address['address'] + \" \" + address['postcode'] for address in addresses]\naddress_strings\n\n['Unit 2 Editions Battersea 15a Parkfield Industrial Estate Culvert Place London SW115BA',\n 'Deliveroo Unit 4 Roman Way Industrial Estate 149 Roman Way Islington N7 8XH',\n 'Unit 3-4 Restwell House Coldhams Road  Cambridge Cambridgeshire CB1 3EW',\n 'Unit 1 Saxon Works 22 Olive Road Hove BN3 7GY',\n 'Unit 1 Scott Hall Mills Scott Hall Street Meanwood Leeds LS7 2HT',\n '20 Acton Park Estate The Vale Acton Ealing W3 7QE',\n '33 Acton Park Estate The Vale Acton London W3 7QE',\n 'Unit 1 Scott Hall Mills Scott Hall Street Meanwood LS7 2HT',\n '86 North Western Street Ardwick Manchester M12 6DY',\n 'Railway Arches 89 To 90  Enid Street London SE16 3RA',\n 'Ground Floor and Basement and Rear Yard 70 New Oxford Street  London WC1A 1EU',\n 'Units 5-7 Yorkton Street Hoxton  Hackney London E2 8NH',\n 'Unit 12 St Josephs Business Park St Josephs Close Hove BN3 7HG',\n 'Units 29 To 30 Clifton Road  Cambridge Cambridgeshire CB1 7EB',\n 'Arch 114 Randall Road London LONDON SE11 5JR',\n 'Unit 2 271 Merton Road London SW18 5JS',\n '145 Ormside Street London SE15 1TF',\n '115 Finchley Road  London NW3 6HY',\n 'Unit 1 Redwood Court Salisbury Street  Nottingham NG7 2BQ',\n 'Unit 3 Crescent Court Business Centre 4C North Crescent Canning Town E16 4TG',\n '2 Phoenix Industrial Estate Apsley Way London NW2 7LN',\n 'Unit 21 Cranford Way Hornsey London N8 9DG',\n 'Unit 6 Great Cambridge Industrial Estate Lincoln Road Enfield EN1 1SH',\n 'Unit 1 Glenfrome House Eastgate Road Eastville Bristol BS5 6XX',\n 'Units 3-4 Dulwich Business Centre Malham Road Forest Hill SE23 1AG',\n 'Cannon Bridge House 1 Cousin Lane London EC4R 3TE',\n '21-23 Oxford Street St Philips Bristol BS2 0QT',\n 'Unit 24 Mitre Bridge Industrial Park Mitre Way London W10 6AU',\n '1 Scotland Street Glasgow G5 8LS',\n 'Unit 4 271 Merton Road London SW18 5JS',\n 'Unit 1 Saxon Works 22 Olive Road Hove BN3 5LE']\n\n\n\neditions_locations = get_editions_locations_near_addresses(address_strings)\n\n\neditions_locations\n\n['london/battersea-york-road',\n 'london/culvert-place-editions',\n 'london/wandsworth-editions',\n 'london/queenstown-road',\n 'london/nine-elms',\n 'london/fish-island-area',\n 'london/whitechapel-editions',\n 'london/hornsey-station',\n 'london/kentish-town',\n 'london/caledonian-road-and-barnsbury',\n 'cambridge/cambridge-south',\n 'cambridge/cambridge-editions',\n 'brighton/brighton-and-hove-editions',\n 'brighton/brighton-centre',\n 'brighton/round-hill',\n 'leeds/editions-in-leeds',\n 'leeds/wortley',\n 'london/acton-editions',\n 'london/maida-vale-editions',\n 'london/hammersmith',\n 'london/white-city-estate',\n 'manchester/editions-ordsall',\n 'manchester/salford-crescent',\n 'london/bsy-1-editions',\n 'london/bsy-2-editions',\n 'london/south-bermondsey-area',\n 'london/honor-oak-park',\n 'london/brent-cross-editions',\n 'london/swiss-cottage-editions',\n 'london/swiss-cottage',\n 'london/mornington-crescent-south',\n 'nottingham/nottingham-editions',\n 'nottingham/nottingham-city-centre',\n 'london/canning-town-editions',\n 'london/canary-wharf',\n 'london/blackwall',\n 'london/west-hendon',\n 'london/jubilee-park-lee-valley',\n 'bristol/bristol-editions-site',\n 'bristol/montpelier-station',\n 'glasgow/glasgow-editions',\n 'glasgow/glasgow-city-centre']\n\n\nWe can now get all the editions kitchens based at all of these sites:\n\neditions = get_restaurants_from_editions_location(editions_locations)\n\n\neditions_df = pd.DataFrame(editions)\n\n\ncolumns_subset = ['name', 'location', 'edition']\n\n\neditions_df = editions_df.drop_duplicates(columns_subset).reset_index(drop=True)\n\n\ncurrent_time = datetime.datetime.now()\n\n\nfilename = 'data/editions_' + current_time.strftime(\"%Y_%m_%d_%Hhr\") + \".csv\"\n\n\neditions_df.to_csv(filename, encoding=\"utf-8\")"
  },
  {
    "objectID": "selenium_utils.html",
    "href": "selenium_utils.html",
    "title": "Selenium",
    "section": "",
    "text": "When scraping with selenium, we should provide relevant headers when making HTTP requests to effectively mimic a popular browser and client, thereby minimizing the chances of a website identifying this software as an automated process which it may block:\nWe will automate a Chrome browser to navigate webpages and extract data. First we need to install appropriate Chrome webdrivers. We should run this cell once before attempting any scrape:\n\nsource\n\ninitialise_driver\n\n initialise_driver (service, headless:bool=False)\n\nInitialises Chrome WebDriver\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nservice\n\n\nInstance of selenium.webdriver.chrome.service.Service\n\n\nheadless\nbool\nFalse\nSet browser to run headless [False] or visble [True]\n\n\n\nLets go ahead an start driving a Chrome Browser. We will set headless to True so we won’t be able to see the browser. You can set headless=False if you’d like to view the browser.\nWe can now try access a webpage and get an HTML element.\n\nsource\n\n\nget_element_by\n\n get_element_by (url:str, driver, css_component:str='id',\n                 css_component_value:str='', timeout:int=15)\n\nGets selenium web element that matches HTML element ID. Waits for element to load before user-defined timeout\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nurl to search for\n\n\ndriver\n\n\ndriver initialised with initialise_driver\n\n\ncss_component\nstr\nid\naccepted values: id, class_name, css_selector\n\n\ncss_component_value\nstr\n\nid of element to wait for when url page renders\n\n\ntimeout\nint\n15\nseconds to wait for element to appear before timeout error\n\n\n\nLets attempt to grab the footer element from the Autonomy ADU homepage:\n\nbase_url = \"https://autonomy.work/adu/\"\nfilter_input = get_element_by(base_url, driver, 'id', 'footer')\n\n\nassert filter_input"
  },
  {
    "objectID": "wayback.html",
    "href": "wayback.html",
    "title": "Wayback",
    "section": "",
    "text": "The following URL searches the Internet Archive for all archived Deliveroo webpages, including search results. With selenium, we can visit this page and filter the results to get all the URLs containing the “deliveroo+editions” filter by adding this text to an input HTML element on the Internet Archive site:\n\nbase_url = \"https://web.archive.org/web/*/https://deliveroo.co.uk/restaurants/*\"\n\nLets go ahead an start driving a Chrome Browser. We will set headless to True so we won’t be able to see the browser. You can set headless=False if you’d like to view the browser.\n\ndriver = initialise_driver(service, True)\n\n\ndriver.get(base_url)\nwait = WebDriverWait(driver, 20)    \nfilter_input = wait.until(EC.presence_of_element_located((By.ID, 'resultsUrl_filter')))\n\nIf the function didn’t return an error then we know the element successfully loaded. Now we’d like to interact with a child of the resultsUrl_filter element and submit the deliveroo+editions filter to get the relevant results:\n\ninput_element = filter_input.find_element(By.TAG_NAME, 'input')\ninput_element.clear()\ninput_element.send_keys('deliveroo+editions')\ntime.sleep(1)\n\nLets now grab all the results from the table:\n\neditions_urls = []\nwhile not editions_urls: \n    table = driver.find_element(By.ID, 'resultsUrl')\n    cells = table.find_elements(By.CLASS_NAME, 'url')\n    editions_urls = []\n    for td in table.find_elements(By.CSS_SELECTOR, 'td.url'):\n        url = td.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n        editions_urls.append(url)\n\n\n# check that table results were scraped:\nassert editions_urls\n\nWe can print all the archived Deliveroo search results filtering for editions:\n\neditions_urls[0:10]\n\n\nGet Captures\nFor each of these historic URLs, we will now get all the captures for each. With these captures we can view versions of these webpages that have been captured over time and extract data from them. Let’s try this with an example url:\n\neditions_urls[5]\n\nInformation on the number of captures including the first and last capture are included conveniently within an element with class_name=captures-range-info\n\ndriver.get(editions_urls[5])\nwait = WebDriverWait(driver, 20)    \nrange_info = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"captures-range-info\")))\n\n\nassert range_info\n\n\nrange_info.text\n\nWe can see that there are captures between 2020 and 2021. Lets extract these years programmatically:\n\neditions_urls[5]\n\n\ndef get_range_info(url:str,  # Wayback calendar view URL\n                  ):\n    \"Returns the range of years for which url captures exist.\"\n    driver = initialise_driver(service,True)\n    driver.get(url)\n    wait = WebDriverWait(driver, 10)    \n    range_info = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"captures-range-info\")))\n    capture_links = range_info.find_elements(By.CSS_SELECTOR, 'a')\n    oldest_capture = capture_links[0].get_attribute('href')\n    latest_capture = capture_links[1].get_attribute('href') \n    start_timestamp = oldest_capture.split('/')[4]\n    end_timestamp = latest_capture.split('/')[4]   \n    start_year = start_timestamp[:4]\n    end_year = end_timestamp[:4]\n    return start_year, end_year, driver\n\nstart_year, end_year, driver = get_range_info(editions_urls[5])\nprint(f\"Captures between {start_year} and {end_year}.\")\n\n\nassert start_year == \"2020\"\nassert end_year == \"2021\"\n\nNow we want to get every capture between these years from the calendar UI. To access the html with captures for a given year, we need to find the clickable element representing that year with class=sparkline-year-label and click on this element to load the html.\n\nyear = start_year\nyear_selector = driver.find_element(By.XPATH, f'//*[contains(@class, \"sparkline-year-label\") and text()=\"{year}\"]')\nassert year_selector\n\n\nyear_selector.click()\n\n\nwait = WebDriverWait(driver, 10) \ncalendar = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"calendar-grid\")))\n\n\ncaptures = calendar.find_elements(By.CSS_SELECTOR, 'a')\ncapture_urls = []\nfor capture in captures:\n    capture_urls.append(capture.get_attribute('href'))\nassert capture_urls\n\nWe’ve now got all the URLs for a single year:\n\ncapture_urls\n\nLets now get every capture for every year, for every url:\nWe may encounter a slight hiccup for all editions_urls with only 1 capture as the url will take us to the capture page rather than the calendar view, therefore we should try to find “capture-range-info” and if not just add the url to our capture_urls:\n\ncapture_urls = []\nfor i, url in enumerate(tqdm(editions_urls)):\n    if \"*\" in url: \n        attempts = 0\n        while attempts &lt; 2: \n            try:\n                start_year, end_year, driver = get_range_info(url)\n                for year in range(int(start_year), int(end_year)+1):\n                    year_selector = driver.find_element(By.XPATH, f'//*[contains(@class, \"sparkline-year-label\") and text()=\"{year}\"]')\n                    year_selector.click()\n                    wait = WebDriverWait(driver, 10) \n                    calendar = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"calendar-grid\")))\n                    captures = calendar.find_elements(By.CSS_SELECTOR, 'a')\n                    for capture in captures:\n                        capture_urls.append(capture.get_attribute('href'))\n                        attempts = 2\n                        break\n            except Exception as e: \n                print(e)\n                attempts += 1\n                time.sleep(1)\n                print(url)\n    else:\n        capture_urls.append(url)\n\n\ncapture_urls\n\nWe can save these urls to a file for access later:\n\nimport json\n\n# with open(\"data/capture_urls.json\", 'w') as f:\n#     json.dump(capture_urls, f) \n\nwith open(\"data/capture_urls.json\", 'r') as f:\n    capture_urls = json.load(f)\n\n# print(capture_urls)\n\nFor each of the captures, we would now like to visit the url and extract all restaurant information. On each URL, there is a grid of restaurants within a &lt;ul&gt; element containing the HomeFeedGrid substring within the class name. We want to first locate all the &lt;li&gt; elements within &lt;ul&gt; without any parent &lt;li&gt; elements as this will give us list items corresponding to each restuarant. For this we can use get_restaurants:\n\n# driver= initialise_driver(service,True)\nurl = capture_urls[0]\nfiltered_tags = get_restaurant_tags(url)\nlen(filtered_tags)\n\nIf we check the number of &lt;li&gt; elements within filtered_li_tags, we should find it equates to the number of restuarants listed on the webpage. Next we want to iterate through these items, locating another unordered list element containing 3 list items (restuarant name, description and delivery details). The element may also contain a link to the restaurant’s Deliveroo url. We can use get_restaurants to get both the tags from before and the metadata all in one step:\n\nget_restaurants(url)\n\nBringing it altogether, lets iterate through the capture urls getting the metadata for each restaurant in each capture. We also need to be careful of rate limits as Wayback limits us to 15 retrieval’s per minute\n\neditions = []\ndriver = initialise_driver(service,False)\nretrievals = 0\nfor i, capture in enumerate(tqdm(capture_urls)):\n    # API limit of 15 retrieval's per minute\n    attempts = 0\n    while attempts &lt; 2:\n        try:\n            metadata = get_restaurants(capture, driver)\n            timestamped_metadata = add_timestamps_to_restaurants(metadata, capture)\n            editions += metadata\n            retrievals += 1\n            attempts = 2\n        except Exception as e:\n            print(e)\n            # reinitialise the driver if error:\n            driver.close()\n            driver = initialise_driver(service,False)\n            attempts += 1\n\n\neditions\n\nLets now process the data such that timestamp and timestamp_url are grouped:\n\neditions_list = []\nfor edition in editions:\n    if editions_list:\n        found=False\n        index = \"\"\n        for i,d in enumerate(editions_list):\n            if d.get('name') == edition['name'] and d.get('location') == edition['location'] and edition['timestamp'] not in d.get('timestamps'):\n                index = i\n                found=True\n                break\n        if found:\n            editions_list[index]['timestamps'].append(edition['timestamp'])\n            editions_list[index]['timestamp_urls'].append(edition['timestamp_url'])\n        else:\n            editions_list.append({'name': edition['name'], 'location': edition['location'], 'timestamps': [edition['timestamp']], 'timestamp_urls': [edition['timestamp_url']], 'restaurant_url': edition['restaurant_url']})\n    else: \n        editions_list.append({'name': editions[0]['name'], 'location': editions[0]['location'], 'timestamps': [editions[0]['timestamp']], 'timestamp_urls': [editions[0]['timestamp_url']], 'restaurant_url': editions[0]['restaurant_url']})\n\n\neditions_list\n\n\neditions_wayback_df = pd.DataFrame.from_dict(editions_list)\n\n\neditions_wayback_df.to_csv('data/editions_wayback.csv', encoding=\"utf-8\")"
  },
  {
    "objectID": "deliveroo_utils.html",
    "href": "deliveroo_utils.html",
    "title": "Deliveroo",
    "section": "",
    "text": "source\n\nget_restaurant_tags\n\n get_restaurant_tags (url:str, driver=None)\n\nReturns all list elements from Deliveroo restaurants webpage corresponding to a restaurant\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nURL for Deliveroo restaurants page\n\n\ndriver\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nget_timestamp\n\n get_timestamp (url:str)\n\nReturns YYYYMMDD timestamp from url of format: https://web.archive.org/web/YYYYMMDD/\n\n\n\n\nType\nDetails\n\n\n\n\nurl\nstr\nURL for Deliveroo\n\n\n\n\nsource\n\n\nadd_timestamps_to_restaurants\n\n add_timestamps_to_restaurants (restaurants, url)\n\n\nsource\n\n\nget_restaurants\n\n get_restaurants (url:str, driver=None)\n\nGets the restaurant name, editions location and Deliveroo restaurant_url for each restaurant on url page.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nURL for Deliveroo restaurants page\n\n\ndriver\nNoneType\nNone\nheadless:bool=True,\n\n\n\n\nmetadata = get_restaurants(\"https://web.archive.org/web/20201019/https://deliveroo.co.uk/restaurants/brighton/brighton-editions?tags=deliveroo+editions\")\nassert metadata == [{'name': 'Oowee Vegan',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/oowee-vegan-editions-bnc?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'Shake Shack',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/shake-shack-editions-bnc?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'Lost Boys Chicken',\n  'location': 'brighton',\n  'edition': 'hove',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/hove/lost-boys-chicken-editions?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'The Athenian',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/the-athenian-editions-bnc-new?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'The Great British Cheesecake Company ',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/the-great-british-cheesecake-company-editions-bnc?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'The Athenian Plant Based',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/the-athenian-plant-based-editions-bnc-new?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'The Ice Cream Store',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/the-ice-cream-store-editions-bnc?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'Pleesecakes - cheesecake',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/pleesecakes-editions-bnc?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'TRIP CBD Store\\t',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/trip-cbd-store-editions-bnc?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'Halo Top',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/halo-top?day=today&geohash=gcpc5qr68ee1&time=ASAP'},\n {'name': 'VIP Very Italian Pizza',\n  'location': 'brighton',\n  'edition': 'brighton-editions',\n  'restaurant_url': 'https://deliveroo.co.uk/menu/brighton/brighton-editions/vip-italy-limited-hove?day=today&geohash=gcpc5qr68ee1&time=1715'},\n {'name': 'A Burgers Veggie Kitchen  by Taster',\n  'location': '',\n  'edition': 'brighton-editions',\n  'restaurant_url': ''},\n {'name': 'Saucybird',\n  'location': '',\n  'edition': 'brighton-editions',\n  'restaurant_url': ''}]\n\nlist index out of range\nCouldn't get metadata for A Burgers Veggie Kitchen  by Taster in https://web.archive.org/web/20201019/https://deliveroo.co.uk/restaurants/brighton/brighton-editions?tags=deliveroo+editions\nlist index out of range\nCouldn't get metadata for Saucybird in https://web.archive.org/web/20201019/https://deliveroo.co.uk/restaurants/brighton/brighton-editions?tags=deliveroo+editions\n\n\n\nsource\n\n\nsearch_deliveroo\n\n search_deliveroo (address:str, driver=None)\n\nSearches Deliveroo for an address, returning webdriver element once search results page has loaded.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naddress\nstr\n\nUK address containing a UK postcode\n\n\ndriver\nNoneType\nNone\nInitialised Selenium webdriver\n\n\n\n\nsource\n\n\nresults_to_editions_url\n\n results_to_editions_url (url:str)\n\nApply deliveroo+editions filter to Deliveroo search results url\n\n\n\n\nType\nDetails\n\n\n\n\nurl\nstr\nDeliveroo search results url\n\n\n\n\nsource\n\n\nget_editions\n\n get_editions (url:str, driver=None)\n\nReturns a list of editions location from all the editions restaurants on url page ie ‘bristol-editions’.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nURL for Deliveroo search results page\n\n\ndriver\nNoneType\nNone\nheadless:bool=True,\n\n\n\n\nsource\n\n\nget_restaurants_from_editions_location\n\n get_restaurants_from_editions_location (editions_list:list)\n\ngets restaurant metadata for all restaurants based at listed editions locations\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\neditions_list\nlist\nlist of editions locations ie [‘london/whitechapel-editions’,‘london/canary-wharf’]\n\n\n\n\nsource\n\n\nget_editions_locations_near_addresses\n\n get_editions_locations_near_addresses (addresses:list, driver=None)\n\nReturns a list of all editions locations found when searching all the restaurants at or near the list of addresses\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naddresses\nlist\n\nlist of address strings to search Deliveroo’s website for\n\n\ndriver\nNoneType\nNone"
  }
]